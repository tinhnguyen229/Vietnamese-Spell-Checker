{"cells":[{"cell_type":"markdown","metadata":{"id":"WqIPDbDCIiTz"},"source":["# Import Library"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s0kJyPshMlj7","outputId":"a994bc36-3f9a-4761-a35a-883a567fd5c9","executionInfo":{"status":"ok","timestamp":1683997392903,"user_tz":-420,"elapsed":7063,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"]}],"source":["pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r5VkXcd9IiT2"},"outputs":[],"source":["import math\n","import random\n","import numpy as np\n","import pandas as pd\n","import re\n","import nltk\n","import pickle\n","import json\n","import tokenize\n","import csv\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R2QepZJbJJpl","outputId":"94d0ec34-4b65-4d52-d208-e444eb2d1975","executionInfo":{"status":"ok","timestamp":1683997418682,"user_tz":-420,"elapsed":24423,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"markdown","metadata":{"id":"iOl7WpXpIiT6"},"source":["# Process data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lwbEdPOSIiT6"},"outputs":[],"source":["def input_data(path):\n","    data = []\n","    with open(path, encoding=\"utf8\") as f:\n","        for line in f:\n","            line = line.lower()\n","            sentences = re.split(\"[.,!?\\:\\-\\(\\)\\\"\\']\", line)\n","            sentences = [item.strip() for item in sentences if item]\n","            for sen in sentences:\n","                data.append(sen)\n","    data = [sen.strip() for sen in data if sen]\n","\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120},"id":"1ys6S2MKIiT7","outputId":"db816523-a7e2-44e9-99ba-c2ab172cc62e","executionInfo":{"status":"ok","timestamp":1683951933912,"user_tz":-420,"elapsed":21,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef input_data(path):\\n    data = []\\n    with open(path, encoding=\"utf8\") as f:\\n        for line in f:\\n            line = line.lower()\\n\\n            # line = re.sub(r\\'[^a-zA-Z0-9\\\\s]\\', \\' \\', line)\\n            line = re.sub(r\\'[.]\\', \\' \\', line)\\n            line = re.sub(r\\'[\\n]\\', \\' \\', line)\\n            line = re.sub(r\\'[,]\\', \\' \\', line)\\n            line = re.sub(r\\'[?]\\', \\' \\', line)\\n            line = re.sub(r\\'[:]\\', \\' \\', line)\\n            line = re.sub(r\\'[\"(,)/\\']\\', \\' \\', line)\\n            line = re.sub(r\\'\\\\s+\\', \\' \\', line)\\n\\n            data.append(line.strip())\\n    return data\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}],"source":["\"\"\"\n","def input_data(path):\n","    data = []\n","    with open(path, encoding=\"utf8\") as f:\n","        for line in f:\n","            line = line.lower()\n","\n","            # line = re.sub(r'[^a-zA-Z0-9\\s]', ' ', line)\n","            line = re.sub(r'[.]', ' ', line)\n","            line = re.sub(r'[\\n]', ' ', line)\n","            line = re.sub(r'[,]', ' ', line)\n","            line = re.sub(r'[?]', ' ', line)\n","            line = re.sub(r'[:]', ' ', line)\n","            line = re.sub(r'[\"(,)/\\']', ' ', line)\n","            line = re.sub(r'\\s+', ' ', line)\n","\n","            data.append(line.strip())\n","    return data\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UgmNjF1xIiT7"},"outputs":[],"source":["# path = \"E:\\\\Khóa luận\\\\Spell_Check_Program\\\\data\\\\\"\n","# data = input_data(path + \"corpus.txt\")\n","# print(type(data))\n","# print(type(data[0]))\n","# for i in range(40,50):\n","#    print(data[i])\n","# data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lb_b29GXIiT8"},"outputs":[],"source":["def tokenize_sentence(sentence):\n","    tokens = nltk.word_tokenize(sentence)\n","    return tokens\n","\n","def tokenize_data(data):\n","    token_of_data = []\n","    for sentence in data:\n","        token_of_sentence = tokenize_sentence(sentence)\n","        token_of_data.append(token_of_sentence)\n","    return token_of_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PILwWQ8yIiT9"},"outputs":[],"source":["#token_of_data = tokenize_data(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EL5lYJebIiT9"},"outputs":[],"source":["#token_of_data[3]"]},{"cell_type":"markdown","metadata":{"id":"4zzUHwTwIiT9"},"source":["# Save/Load corpus(tokenized)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yQ23ifhIiT9"},"outputs":[],"source":["# import pickle\n","# path = \"E:\\\\Khóa luận\\\\Spell_Check_Program\\\\data\\\\train_data\\\\\"\n","# path = \"C:\\\\Users\\\\thaib\\\\Downloads\\\\Khóa luận\\\\data\\\\\""]},{"cell_type":"markdown","metadata":{"id":"Lv_cnSA6IiT-"},"source":["## Save"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x1JCfDCsIiT-"},"outputs":[],"source":["def save_corpus(path, corpus):\n","    with open(path, 'w', encoding=\"utf8\") as tsv_file:\n","        tsv_writer = csv.writer(tsv_file, delimiter='\\t', lineterminator='\\n')\n","        for sentence in corpus:\n","            for word in sentence:\n","                tsv_writer.writerow([word])\n","            tsv_writer.writerow([\" \"])\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ppCwU84XIiT-"},"outputs":[],"source":["# save_corpus(path+\"corpus.tsv\", token_of_data)"]},{"cell_type":"markdown","metadata":{"id":"uuUafN46IiT-"},"source":["## Load"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tWIphnyZIiT-"},"outputs":[],"source":["def load_corpus(path):\n","    token_of_data = []\n","    with open(path, \"r\", encoding=\"utf-8\") as tsv_file:\n","        read = csv.reader(tsv_file, delimiter=\"\\t\")\n","        sentence = []\n","        for row in read:\n","            if row[0].isspace():\n","                token_of_data.append(sentence)\n","                sentence = []\n","            else:\n","                sentence.append(row[0])\n","\n","    return token_of_data\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9LeBxpA4IiT_"},"outputs":[],"source":["# token_of_data = load_corpus(path+\"corpus.tsv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0KlslVbEIiT_"},"outputs":[],"source":["#num_sens = 0\n","#num_tokens = 0\n","#for sen in token_of_data:\n","#    num_sens += 1\n","#    num_tokens += len(sen)\n","#print(num_sens) # 20152478\n","\n","#print(num_tokens) # 143862988"]},{"cell_type":"markdown","metadata":{"id":"QlXU8K65IiT_"},"source":["# Save/load vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vdM_HhKeIiT_"},"outputs":[],"source":["def input_vocab(path_vocab):\n","    vocab = dict()\n","    arr = []\n","    loop = 0\n","    with open(path_vocab, encoding=\"utf8\") as f:\n","        for line in f:\n","            line = line.replace(\"\\n\", \"\")\n","            #print(line)\n","            if tuple(line.split()) in vocab:\n","                vocab[tuple(line.split())] += 1\n","            else:\n","                vocab[tuple(line.split())] = 1\n","    return vocab\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btkgF77qIiT_"},"outputs":[],"source":["# vocab = input_vocab('E:\\\\Khóa luận\\\\Spell_Check_Program\\\\data\\\\train_data\\\\TuDienTV74K.txt')\n","vocab = input_vocab('/content/drive/MyDrive/Colab Notebooks/TuDienTV74K.txt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E-Op1HKIIiT_","outputId":"35343c87-65b3-4d08-9bb8-1f021e6a6f25","executionInfo":{"status":"ok","timestamp":1683951934331,"user_tz":-420,"elapsed":8,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":18}],"source":["vocab['nhà', 'cửa']\n","('nhà', 'cửa') in vocab"]},{"cell_type":"markdown","metadata":{"id":"xmo6k8BMIiUA"},"source":["# N-grams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"58k4a3s-IiUA"},"outputs":[],"source":["def n_grams(text, n):\n","    # text = text.lower()\n","    # text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n","    words = text.split() # text:list\n","    grams = []\n","    end = len(words) - n + 1\n","    for i in range(0, end):\n","        item = tuple([words[i] for i in range(i, i+n)]) # convert list to tuple\n","        grams.append(item)\n","    return grams\n","\n","# n_grams(token_of_data[0],2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TOertklmIiUA"},"outputs":[],"source":["\"\"\"\n","Get n_gram and frequency of n_gram.\n","\n","data:list [[]]\n","n: unigram, bigram, trigram, 4-gram,...\n","return dict()\n","\"\"\"\n","def count_n_grams(data, n):\n","   number_n_grams = dict() # dictionary with {key:value} <-> {n_gram:times}\n","   for i in range(len(data)):\n","       sentences = tuple(data[i])\n","\n","       end = len(sentences) - n + 1\n","       for j in range(0, end):\n","           # Get the n-gram from j to j+n\n","           n_gram = tuple([sentences[k] for k in range(j, j+n)])\n","\n","           # If n_gram(key) not in dict then (value) = 1\n","           if not(n_gram in number_n_grams.keys()):\n","               number_n_grams[n_gram] = 1\n","\n","           else:\n","               # If n_gram(key) in dict then (value) increase 1\n","               number_n_grams[n_gram] += 1\n","\n","   return number_n_grams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_dUCqn7IiUB"},"outputs":[],"source":["# unigram = count_n_grams(token_of_data[0:len(token_of_data)], 1)\n","# bigram = count_n_grams(token_of_data[0:len(token_of_data)], 2)\n","# trigram = count_n_grams(token_of_data[0:len(token_of_data)], 3)\n","# unigams = count_n_grams(token_of_data,1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SLY6_OI6IiUB"},"outputs":[],"source":["# unigrams = unigams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cajMm2xSIiUB"},"outputs":[],"source":["# bigrams = count_n_grams(token_of_data, 2)"]},{"cell_type":"markdown","metadata":{"id":"GdEI_9t-IiUD"},"source":["# Save and load n-gram counts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yGZPguXSIiUD"},"outputs":[],"source":["# path = \"E:\\\\Khóa luận\\\\Spell_Check_Program\\\\data\\\\train_data\\\\\"\n","# path = \"C:\\\\Users\\\\thaib\\\\Downloads\\\\Khóa luận\\\\data\\\\\"\n","path = \"/content/drive/MyDrive/Colab Notebooks/\""]},{"cell_type":"markdown","metadata":{"id":"hNiWk4FxIiUD"},"source":["## Save"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7P9tArzqIiUD"},"outputs":[],"source":["# file = open(path + \"unigrams.pickle\",\"wb\")\n","# pickle.dump(unigrams,file)\n","# file.close()\n","\n","# file = open(path + \"bigrams.pickle\",\"wb\")\n","# pickle.dump(bigrams,file)\n","# file.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"761Yt4FiIiUD"},"outputs":[],"source":["#file = open(path + \"trigram_word.pickle\",\"wb\")\n","#pickle.dump(trigram, file)\n","#file.close()"]},{"cell_type":"markdown","metadata":{"id":"rLiio2DLIiUD"},"source":["## Load"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x5SVThy_IiUE"},"outputs":[],"source":["file = open(path + \"unigrams.pickle\",\"rb\")\n","unigrams = pickle.load(file)\n","file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bAxWmJ_CIiUE","outputId":"d3e5c5c0-0e4c-4b9f-cce1-30da8dfa7573","executionInfo":{"status":"ok","timestamp":1683997420850,"user_tz":-420,"elapsed":5,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["269002"]},"metadata":{},"execution_count":10}],"source":["len(unigrams)\n","#print(unigrams)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1PiZc4qiIiUE"},"outputs":[],"source":["file = open(path + \"bigrams.pickle\",\"rb\")\n","bigrams = pickle.load(file)\n","file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TULToJEyIiUE","outputId":"04549547-a8ab-47e5-884a-6a9480f5b7e0","executionInfo":{"status":"ok","timestamp":1683997429997,"user_tz":-420,"elapsed":32,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["5139553"]},"metadata":{},"execution_count":12}],"source":["len(bigrams)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ar6AIcQKIiUE"},"outputs":[],"source":["# word_counts = [unigram_word, bigram_word, trigram_word]\n","word_counts = [unigrams, bigrams]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1CCqTXGIiUE"},"outputs":[],"source":["# file = open(path + \"trigram_word_old.pickle\",\"rb\")\n","# file = open(path + \"trigram_word.pickle\",\"rb\")\n","# trigram_word = pickle.load(file)\n","# file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JO_Mr_hUIiUF"},"outputs":[],"source":["# bigrams"]},{"cell_type":"markdown","metadata":{"id":"JI2fEiLlIiUF"},"source":["# Compute Probability"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"713iTv39IiUF"},"outputs":[],"source":["\"\"\"\n","ngram: unigram, bigram or trigram. Ex: ('tôi',) ; ('tôi', 'là'); ('tôi', 'là', 'sinh')\n","counts: [unigram_word, bigram_word, trigram_word] <-> counts[i]:dict()\n","return float : probability of 1 n-gram\n","\"\"\"\n","def get_probability(ngram, counts):\n","    ngram = tuple(ngram)\n","    if len(ngram) == 1:\n","        unigrams = counts[0]\n","        if ngram in unigrams:\n","            # probability = float(unigrams[ngram]) / float(sum(unigrams.values()))\n","            probability = float(unigrams.get(ngram)) / float(sum(unigrams.values()))\n","        else:\n","            probability = 0\n","\n","    elif len(ngram) == 2:\n","        unigrams = counts[0]\n","        bigrams = counts[1]\n","\n","        w1 = ngram[0]\n","        w2 = ngram[1]\n","\n","        if ngram in bigrams:\n","            # probability = float(bigrams[ngram]) / float(unigrams[(w1, )])\n","            probability = float(bigrams.get(ngram)) / float(unigrams.get((w1, )))\n","        else:\n","            probability = 0\n","\n","    else:\n","        bigrams = counts[1]\n","        trigrams = counts[2]\n","\n","        w1 = ngram[0]\n","        w2 = ngram[1]\n","        w3 = ngram[2]\n","\n","        if ngram in trigrams:\n","            # probability = float(trigrams[ngram]) / float(bigrams[(w1, w2)])\n","            probability = float(trigrams.get(ngram)) / float(bigrams.get((w1, w2)))\n","\n","        else:\n","            probability = 0\n","\n","    return probability\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9StNyIyiIiUF"},"outputs":[],"source":["def add_one_smoothing(ngram, counts):\n","    ngram = tuple(ngram)\n","    if len(ngram) == 1:\n","        unigrams = counts[0]\n","        N = sum(unigrams.values())\n","        V = len(unigrams)\n","        if ngram in unigrams:\n","            probability = float(unigrams[ngram]+1) / float(N+V)\n","        else:\n","            probability = 1 / float(N+V)\n","    elif len(ngram) == 2:\n","        unigrams = counts[0]\n","        bigrams = counts[1]\n","        V = len(unigrams)\n","        if ngram in bigrams:\n","            probability = float(bigrams[ngram]+1) / float(unigrams[ngram[0],]+V)\n","        else:\n","            #if [ngram[0],] in unigrams:\n","            if ngram[0:1] in unigrams:\n","                probability = 1 / float(unigrams[ngram[0],]+V)\n","            else:\n","                probability = 1/float(V)\n","    else:\n","        unigrams = counts[0]\n","        bigrams = counts[1]\n","        trigrams = counts[2]\n","        V = len(unigrams)\n","        if ngram in trigrams:\n","            probability = (trigrams[ngram]+1) / float(bigrams[ngram[0:2]]+V)\n","        else:\n","            if ngram[0:2] in trigrams:\n","                probability = 1 / float(bigrams[ngram[0:2]] + V)\n","            else:\n","                probability = 1 / float(V)\n","    return probability\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AXq1et5XIiUF","outputId":"75404262-1922-4d7b-9994-5744430cce7d","executionInfo":{"status":"ok","timestamp":1683997430002,"user_tz":-420,"elapsed":33,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0.008771253030346099\n","0.0016051087331722471\n","0.004193991006499619\n","0.000632308241234627\n","-5.474102490877713\n"]},{"output_type":"execute_result","data":{"text/plain":["-7.36613355924334"]},"metadata":{},"execution_count":18}],"source":["print(get_probability(n_grams(\"tôi yêu\", 2)[0], word_counts))\n","print(get_probability(n_grams(\"yêu việt\", 2)[0], word_counts))\n","print(add_one_smoothing(n_grams(\"tôi yêu\", 2)[0], word_counts))\n","print(add_one_smoothing(n_grams(\"yêu việt\", 2)[0], word_counts))\n","0.004193991006499619*0.000632308241234627\n","print(math.log(0.004193991006499619))\n","math.log(0.000632308241234627)"]},{"cell_type":"markdown","metadata":{"id":"0SJ10e-aIiUG"},"source":["# Probabilites of N-gram"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"gHqXAJX6IiUG","outputId":"d740bb55-a0e3-4504-b11d-7ca977c28836","executionInfo":{"status":"ok","timestamp":1683997430002,"user_tz":-420,"elapsed":30,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef language_model(counts, n):\\n    unigrams = counts[0]\\n    bigrams = counts[1]\\n    \\n    unigrams_prob = dict()\\n    bigrams_prob = dict()\\n    \\n    if n == 1:\\n        for uni in unigrams:\\n            unigrams_prob[uni] = add_one_smoothing([uni[0]], counts)\\n        return unigrams_prob\\n    else:\\n        for big in bigrams:\\n            bigrams_prob[big] = add_one_smoothing([big[0], big[1]] ,counts)\\n        return bigrams_prob\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}],"source":["\"\"\"\n","def language_model(counts, n):\n","    unigrams = counts[0]\n","    bigrams = counts[1]\n","    trigrams = counts[2]\n","\n","    unigrams_prob = dict()\n","    bigrams_prob = dict()\n","    trigrams_prob = dict()\n","\n","    if n == 1:\n","        for uni in unigrams:\n","            unigrams_prob[uni] = add_one_smoothing([uni[0]], counts)\n","        return unigrams_prob\n","    elif n == 2:\n","        for big in bigrams:\n","            bigrams_prob[big] = add_one_smoothing([big[0], big[1]] ,counts)\n","        return bigrams_prob\n","    else:\n","        for tri in trigrams:\n","            trigrams_prob[tri] = add_one_smoothing([tri[0], tri[1], tri[2]], counts)\n","        return trigrams_prob\n","\"\"\"\n","'''\n","def language_model(counts, n):\n","    unigrams = counts[0]\n","    bigrams = counts[1]\n","\n","    unigrams_prob = dict()\n","    bigrams_prob = dict()\n","\n","    if n == 1:\n","        for uni in unigrams:\n","            unigrams_prob[uni] = add_one_smoothing([uni[0]], counts)\n","        return unigrams_prob\n","    else:\n","        for big in bigrams:\n","            bigrams_prob[big] = add_one_smoothing([big[0], big[1]] ,counts)\n","        return bigrams_prob\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5TtUSWtIiUG"},"outputs":[],"source":["# unigrams_prob = language_model(word_counts, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"avDIflKLIiUG"},"outputs":[],"source":["# bigrams_prob = language_model(word_counts, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ydDeQ9m7IiUG"},"outputs":[],"source":["# trigrams_word_prob = language_model(word_counts, 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UlgwKiIFIiUG"},"outputs":[],"source":["# bigrams_prob['việt','nam']"]},{"cell_type":"markdown","metadata":{"id":"t5FYVTVSIiUH"},"source":["# Find recommended word after n-1 gram"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Imkhy2yIIiUH"},"outputs":[],"source":["\"\"\"\n","ngram: unigram or bigram. Ex: (w1,) or (w1,w2)\n","counts: [unigram_word, bigram_word, trigram_word]\n","return: tuple (max probability, recommended word)\n","\"\"\"\n","def find_recommend_word(ngram, counts):\n","    recommended_word = []\n","    unigram = counts[0]\n","    for word in unigram:\n","        if len(ngram) == 2:\n","            tokens = tuple([ngram[0], ngram[1], str(word[0])]) #word[0] = str\n","        else:\n","            tokens = tuple([ngram[0], word[0]])\n","\n","        prob = add_one_smoothing(tokens, counts)\n","        recommended_word.append(tuple([prob, word[0]]))\n","\n","    recommended_word.sort(key=lambda x: x[0], reverse = True) # sort a list of tuples with probablity parameter\n","    return recommended_word\n","    # return recommended_word[0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9gsUMIrgIiUH"},"outputs":[],"source":["# find_recommend_word([\"xin\"], word_counts)"]},{"cell_type":"markdown","metadata":{"id":"MrLZnqDoIiUH"},"source":["# Save and load n-gram probabilities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFY5tD6PIiUH"},"outputs":[],"source":["#path = \"E:\\\\Khóa luận\\\\Spell_Check_Program\\\\data\\\\\"\n","#path = \"C:\\\\Users\\\\thaib\\\\Downloads\\\\Khóa luận\\\\data\\\\\""]},{"cell_type":"markdown","metadata":{"id":"8yX3G4HUIiUH"},"source":["## Save"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"COX46N2KIiUI","outputId":"be7b61b8-2f75-4902-e17c-14ff55044aa7","executionInfo":{"status":"ok","timestamp":1683997430005,"user_tz":-420,"elapsed":29,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfile = open(path + \"unigrams_prob.pickle\",\"wb\")\\npickle.dump(unigrams_prob,file)\\nfile.close()\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}],"source":["'''\n","file = open(path + \"unigrams_prob.pickle\",\"wb\")\n","pickle.dump(unigrams_prob,file)\n","file.close()\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"NKzXNGVnIiUI","outputId":"be9e44ee-e8c9-4867-eeb6-4bd16e60061c","executionInfo":{"status":"ok","timestamp":1683997430005,"user_tz":-420,"elapsed":28,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfile = open(path + \"bigrams_prob.pickle\",\"wb\")\\npickle.dump(bigrams_prob,file)\\nfile.close()\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":28}],"source":["'''\n","file = open(path + \"bigrams_prob.pickle\",\"wb\")\n","pickle.dump(bigrams_prob,file)\n","file.close()\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEDd01-IIiUI"},"outputs":[],"source":["# file = open(path + \"trigrams_word_prob.pickle\",\"wb\")\n","# pickle.dump(trigrams_word_prob,file)\n","# file.close()"]},{"cell_type":"markdown","metadata":{"id":"2-Q_LFYaIiUI"},"source":["## Load"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"oKBf-smpIiUI","outputId":"1aef8647-c6b2-4ff2-d605-c4c8ed7b238c","executionInfo":{"status":"ok","timestamp":1683997430006,"user_tz":-420,"elapsed":28,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfile = open(path + \"unigrams_prob.pickle\",\"rb\")\\nunigrams_prob = pickle.load(file)\\nfile.close()\\n\\nfile = open(path + \"bigrams_prob.pickle\",\"rb\")\\nbigrams_prob = pickle.load(file)\\nfile.close()\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}],"source":["'''\n","file = open(path + \"unigrams_prob.pickle\",\"rb\")\n","unigrams_prob = pickle.load(file)\n","file.close()\n","\n","file = open(path + \"bigrams_prob.pickle\",\"rb\")\n","bigrams_prob = pickle.load(file)\n","file.close()\n","'''\n","# file = open(path + \"trigrams_word_prob.pickle\",\"rb\")\n","# trigrams_word_prob = pickle.load(file)\n","# file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y25JFSULIiUI"},"outputs":[],"source":["#word_probs = [unigrams_word_prob, bigrams_word_prob, trigrams_word_prob]\n","# word_probs = [unigrams_prob, bigrams_prob]"]},{"cell_type":"markdown","metadata":{"id":"2IE0so5EIiUI"},"source":["# Load TSV file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"52Qqjh9FIiUJ"},"outputs":[],"source":["def read_tsv(path_tsv):\n","    i_data = [] # incorrected sentences\n","    c_data = [] # corrected sentences\n","    label_data = []\n","    with open(path_tsv, \"r\", encoding=\"utf-8\") as tsv_file:\n","        read = csv.reader(tsv_file, delimiter=\"\\t\")\n","\n","        i_sentence = []\n","        c_sentence = []\n","        label = []\n","        for row in read:\n","            #print(row[0],row[1],row[2])\n","\n","            if row[0].isspace():\n","                i_data.append(i_sentence)\n","                label_data.append(label)\n","                c_data.append(c_sentence)\n","                i_sentence = []\n","                label = []\n","                c_sentence = []\n","            else:\n","                i_sentence.append(row[0])\n","                label.append(row[1])\n","                c_sentence.append(row[2])\n","\n","\n","    return i_data, c_data, label_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vG4UTUQSIiUJ","colab":{"base_uri":"https://localhost:8080/","height":154},"executionInfo":{"status":"ok","timestamp":1683997430006,"user_tz":-420,"elapsed":27,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}},"outputId":"0a6ea7f2-94a5-486c-903b-6f3dda6c5471"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\npath_tsv = \"E:\\\\Khóa luận\\\\Spell_Check_Program\\\\data\\\\test_data\\\\test1.tsv\"\\npath_tsv1 = \"E:\\\\Khóa luận\\\\Spell_Check_Program\\\\data\\\\test_data\\\\test1.1.tsv\"\\npath_tsv2 = \"E:\\\\Khóa luận\\\\Spell_Check_Program\\\\data\\\\test_data\\\\test2.tsv\"\\npath_tsv3 = \"E:\\\\Khóa luận\\\\Spell_Check_Program\\\\data\\\\test_data\\\\test3.tsv\"\\npath_tsv4 = \"E:\\\\Khóa luận\\\\Spell_Check_Program\\\\data\\\\test_data\\\\test4.tsv\"\\n\\npath_tsv = \"/content/drive/MyDrive/Colab Notebooks/data/test_data/test1.tsv\"\\npath_tsv1 = \"/content/drive/MyDrive/Colab Notebooks/data/test_data/test1.1.tsv\"\\npath_tsv2 = \"/content/drive/MyDrive/Colab Notebooks/data/test_data/test2.tsv\"\\npath_tsv3 = \"/content/drive/MyDrive/Colab Notebooks/data/test_data/test3.tsv\"\\npath_tsv4 = \"/content/drive/MyDrive/Colab Notebooks/data/test_data/test4.tsv\"\\npath_tsv5 = \"/content/drive/MyDrive/Colab Notebooks/data/test_data/small_test.tsv\"\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":33}],"source":["'''\n","path_tsv = \"E:\\\\Khóa luận\\\\Spell_Check_Program\\\\data\\\\test_data\\\\test1.tsv\"\n","path_tsv1 = \"E:\\\\Khóa luận\\\\Spell_Check_Program\\\\data\\\\test_data\\\\test1.1.tsv\"\n","path_tsv2 = \"E:\\\\Khóa luận\\\\Spell_Check_Program\\\\data\\\\test_data\\\\test2.tsv\"\n","path_tsv3 = \"E:\\\\Khóa luận\\\\Spell_Check_Program\\\\data\\\\test_data\\\\test3.tsv\"\n","path_tsv4 = \"E:\\\\Khóa luận\\\\Spell_Check_Program\\\\data\\\\test_data\\\\test4.tsv\"\n","\n","path_tsv = \"/content/drive/MyDrive/Colab Notebooks/data/test_data/test1.tsv\"\n","path_tsv1 = \"/content/drive/MyDrive/Colab Notebooks/data/test_data/test1.1.tsv\"\n","path_tsv2 = \"/content/drive/MyDrive/Colab Notebooks/data/test_data/test2.tsv\"\n","path_tsv3 = \"/content/drive/MyDrive/Colab Notebooks/data/test_data/test3.tsv\"\n","path_tsv4 = \"/content/drive/MyDrive/Colab Notebooks/data/test_data/test4.tsv\"\n","path_tsv5 = \"/content/drive/MyDrive/Colab Notebooks/data/test_data/small_test.tsv\"\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7zNHMOeNIiUJ","colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"status":"ok","timestamp":1683997430007,"user_tz":-420,"elapsed":27,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}},"outputId":"b0fc2cff-0327-42f2-febf-9a4152ba4b1d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nerror_data ,corrected_data, labels = read_tsv(path_tsv)\\nerror_data1 ,corrected_data1, labels1 = read_tsv(path_tsv1)\\nerror_data2 ,corrected_data2, labels2 = read_tsv(path_tsv2)\\nerror_data3 ,corrected_data3, labels3 = read_tsv(path_tsv3)\\nerror_data4 ,corrected_data4, labels4 = read_tsv(path_tsv4)\\nerror_data5 ,corrected_data5, labels5 = read_tsv(path_tsv5)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":34}],"source":["'''\n","error_data ,corrected_data, labels = read_tsv(path_tsv)\n","error_data1 ,corrected_data1, labels1 = read_tsv(path_tsv1)\n","error_data2 ,corrected_data2, labels2 = read_tsv(path_tsv2)\n","error_data3 ,corrected_data3, labels3 = read_tsv(path_tsv3)\n","error_data4 ,corrected_data4, labels4 = read_tsv(path_tsv4)\n","error_data5 ,corrected_data5, labels5 = read_tsv(path_tsv5)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"id":"p7MK_ASqIiUJ","outputId":"54bd230e-0753-4d3e-89c4-12009880eb13","executionInfo":{"status":"ok","timestamp":1683997430007,"user_tz":-420,"elapsed":26,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef statistic_of_err_data(error_data, corrected_data, labels):\\n    print(\"Number of sentences: \", len(error_data))\\n    num_of_tokens = 0\\n    for sen in error_data:\\n        num_of_tokens += len(sen)\\n    print(\"Number of tokens: \", num_of_tokens)\\n\\n    num_of_errors = 0\\n    for label in labels:\\n        num_of_errors += label.count(\"i\")\\n    print(\"Number of error tokens: \", num_of_errors)\\n    print(\"Error rate: \", (float(num_of_errors/num_of_tokens))*100,\\'%\\')\\nstatistic_of_err_data(error_data ,corrected_data, labels)\\nprint(\\'---------------------------------------------\\')\\nstatistic_of_err_data(error_data1 ,corrected_data1, labels1)\\nprint(\\'---------------------------------------------\\')\\nstatistic_of_err_data(error_data2 ,corrected_data2, labels2)\\nprint(\\'---------------------------------------------\\')\\nstatistic_of_err_data(error_data3 ,corrected_data3, labels3)\\nprint(\\'---------------------------------------------\\')\\nstatistic_of_err_data(error_data4 ,corrected_data4, labels4)\\nprint(\\'---------------------------------------------\\')\\nstatistic_of_err_data(error_data5 ,corrected_data5, labels5)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":35}],"source":["\"\"\"\n","def statistic_of_err_data(error_data, corrected_data, labels):\n","    print(\"Number of sentences: \", len(error_data))\n","    num_of_tokens = 0\n","    for sen in error_data:\n","        num_of_tokens += len(sen)\n","    print(\"Number of tokens: \", num_of_tokens)\n","\n","    num_of_errors = 0\n","    for label in labels:\n","        num_of_errors += label.count(\"i\")\n","    print(\"Number of error tokens: \", num_of_errors)\n","    print(\"Error rate: \", (float(num_of_errors/num_of_tokens))*100,'%')\n","statistic_of_err_data(error_data ,corrected_data, labels)\n","print('---------------------------------------------')\n","statistic_of_err_data(error_data1 ,corrected_data1, labels1)\n","print('---------------------------------------------')\n","statistic_of_err_data(error_data2 ,corrected_data2, labels2)\n","print('---------------------------------------------')\n","statistic_of_err_data(error_data3 ,corrected_data3, labels3)\n","print('---------------------------------------------')\n","statistic_of_err_data(error_data4 ,corrected_data4, labels4)\n","print('---------------------------------------------')\n","statistic_of_err_data(error_data5 ,corrected_data5, labels5)\n","\"\"\"\n"]},{"cell_type":"markdown","metadata":{"id":"m6MCm0QhIiUK"},"source":["# Edit distance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4doNRBFnIiUK","outputId":"3f194ac4-e810-44ef-bd47-5324fe1845b9","executionInfo":{"status":"ok","timestamp":1683997430563,"user_tz":-420,"elapsed":2,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["1\n"]}],"source":["from nltk.metrics.distance import edit_distance\n","# Edit distance returns the number of changes to transform one word to another\n","print(edit_distance(\"yêu\", \"yeu\"))"]},{"cell_type":"markdown","metadata":{"id":"8TyZsU6RIiUL"},"source":["# Detect spell error"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7kHWCG8SIiUL"},"outputs":[],"source":["def detect_misspell(sentence, word_counts, threshold=10):\n","    unigrams = word_counts[0]\n","    bigrams = word_counts[1]\n","    if len(tuple(sentence.strip().split())) == 1:\n","        single_word = tuple(sentence.strip().split())\n","        if single_word not in unigrams or unigrams[single_word] < 50 :\n","            return [tuple([single_word[0], 0])]\n","        else:\n","            return []\n","    if len(tuple(sentence.strip().split())) == 2:\n","        two_words =  tuple(sentence.strip().split())\n","        wrong = []\n","        if two_words not in bigrams:\n","            wrong.append(two_words)\n","        else:\n","            if -math.log(add_one_smoothing(two_words, word_counts)) > threshold:\n","                wrong.append(two_words)\n","        if len(wrong) != 0:\n","            if (wrong[0][0],) in unigrams and (wrong[0][1],) in unigrams:\n","                if unigrams[wrong[0][0],] < unigrams[wrong[0][1],]:\n","                    return [tuple([wrong[0][0], 0])]\n","                else:\n","                    return [tuple([wrong[0][1], 1])]\n","            else:\n","                if (wrong[0][0],) in unigrams:\n","                    return [tuple([wrong[0][1], 1])]\n","                else:\n","                    return [tuple([wrong[0][0], 0])]\n","        else:\n","            return []\n","\n","    #threshold = 10\n","    flags = []\n","    ngrams = []\n","    idx_of_gram = 0\n","    for i in n_grams(sentence,2):\n","        ngrams.append([i, idx_of_gram])\n","        #print(i, ':', word_counts[1][i])\n","        idx_of_gram += 1\n","    for idx in range(len(ngrams)):\n","        if idx == 0:\n","            if -math.log(add_one_smoothing(ngrams[idx][0], word_counts)) > threshold:\n","                flags.append(ngrams[idx])\n","        elif idx == len(ngrams)-1:\n","            if -math.log(add_one_smoothing(ngrams[idx][0], word_counts)) > threshold:\n","                flags.append(ngrams[idx])\n","        else:\n","            if (-math.log(add_one_smoothing(ngrams[idx][0], word_counts))) > threshold and\\\n","                (-math.log(add_one_smoothing(ngrams[idx+1][0], word_counts))) > threshold:\n","                flags.append(ngrams[idx])\n","    #print(flags)\n","    if len(flags) == 0:\n","        #print(\"No error\")\n","        return []\n","\n","    #print(flags)\n","\n","    suspected = []\n","    if len(flags) > 0:\n","        for j in range(0, len(flags)):\n","            sub_arr = []\n","            if j < len(flags)-1:\n","                if flags[j][1] == flags[j+1][1]-1:\n","                    sub_arr.extend([flags[j][0], flags[j+1][0], math.ceil((flags[j][1] + flags[j+1][1])/2)])\n","                    if j+1 == len(flags)-1:\n","                        continue\n","                    #j += 2\n","                if flags[j][1] != flags[j-1][1]+1 and flags[j][1] != flags[j+1][1]-1:\n","                    #if j != 0:\n","                    sub_arr.extend([flags[j][0], flags[j][1]])\n","                    #else:\n","                    #    sub_arr.extend([flag[j][0]])\n","            if j == len(flags) -1:\n","                if flags[j][1] == flags[j-1][1]+1:\n","                    sub_arr.extend([flags[j-1][0], flags[j][0], math.ceil((flags[j][1] + flags[j-1][1])/2)])\n","                else:\n","                    sub_arr.extend([flags[j][0], flags[j][1]])\n","            suspected.append(sub_arr)\n","    else:\n","        suspected.append([flags[0][0], flags[0][1]])\n","\n","    #print(suspected)\n","\n","    suspected = [sus for sus in suspected if len(sus)!=0]\n","    err = []\n","    for item in suspected:\n","        if len(item) == 2:\n","            if (item[0][0],) in unigrams and (item[0][1],) in unigrams:\n","                if unigrams[item[0][0],] < unigrams[item[0][1],]:\n","\n","                    err.append(tuple([item[0][0], item[1]]))\n","                else:\n","                    err.append(tuple([item[0][1], item[1]+1]))\n","            else:\n","                if (item[0][0],) not in unigrams:\n","                    err.append(tuple([item[0][0], item[1]]))\n","                else:\n","                    err.append(tuple([item[0][0], item[1]+1]))\n","        else:\n","            err.append(tuple([item[0][1], item[2]]))\n","    return err\n",""]},{"cell_type":"markdown","metadata":{"id":"D3QwlnAxIiUN"},"source":["# Pre-process input data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lh723cjNIiUN"},"outputs":[],"source":["def sentences_split(sentences):\n","    upper_alphabet = ['A', 'B', 'C', 'D', 'Đ', 'E', 'Ê', 'F', 'G', 'H', 'I', 'J', 'K',\n","                  'L', 'M', 'N', 'O', 'Ô', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n","    numeric = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n","    #stop_word = [\"những\" ,\"các\", \"vẫn\", \"và\", \"đã\", \"sẽ\", \"đang\", \"vừa\",\\\n","    #                \"mới\", \"từng\", \"là\", \"của\", \"bằng\", \"vì\", \"bởi\", \"cùng\", \"với\", \"nếu\", \"tuy\", \"nên\",\"ra\",\"nhưng\", \"đó\", \"do\", \"để\", \"trở\",]\n","    stop_word = []\n","\n","    data = []\n","    spl = nltk.word_tokenize(sentences)\n","    #print(spl)\n","    idx = 0\n","    sentence = []\n","    for item in spl:\n","        if item in '[\"”%?()[]:;.,\"-``!...“‘’:....]'\\\n","            or item[0] in upper_alphabet\\\n","            or item[0] in numeric\\\n","            or item in stop_word:\n","            data.append(tuple([\" \".join(sentence).strip(), idx-1, 's'])) # s ~ suspicious\n","            data.append(tuple([item, idx, 'c'])) # tuple(special character, index, label \"c\")\n","            sentence = []\n","        else:\n","            sentence.append(item)\n","        idx += 1\n","    data.append(tuple([\" \".join(sentence).strip(), idx-1, 's']))\n","    for i in data:\n","        if len(i[0]) == 0:\n","            data.remove(i)\n","    #print(data)\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sZ-lZp7FM8ab","outputId":"475b3c29-8e50-4d52-ce1e-4df29bd4f9cb","executionInfo":{"status":"ok","timestamp":1683997435915,"user_tz":-420,"elapsed":969,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":39}],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hzrA0k8BIiUN"},"outputs":[],"source":["def vnspell_Detector(sentences, word_counts, vocabulary, threshold = 10):\n","    input = sentences_split(sentences)\n","    #print(input)\n","    result = []\n","    for sen in input:\n","        if sen[2] == 'c':\n","            result.append(([sen[0], 'c']))\n","        else:\n","            error = detect_misspell(sen[0], word_counts, threshold)\n","            #print(error)\n","            if len(error) == 0:\n","                for word in sen[0].split():\n","                    result.append(([word, 'c']))\n","            else:\n","                err_sentence = []\n","                for word in sen[0].strip().split():\n","                    err_sentence.append([word, 'c'])\n","                for e in error:\n","                    err_sentence[e[1]][1] = 'i'\n","                #print(err_sentence)\n","                result.extend(err_sentence)\n","    bigrams = word_counts[1]\n","\n","    #threshold = 12\n","    for i in range(0, len(result)):\n","        if result[i][1] == 'i':\n","            if i==0:\n","                if (result[i][0], result[i+1][0]) in bigrams:\n","                    if -math.log(add_one_smoothing((result[i][0], result[i+1][0]), word_counts))  < threshold or\\\n","                        (result[i][0], result[i+1][0]) in vocabulary:\n","                        result[i][1] = 'c'\n","            if i==len(result) - 1:\n","                if (result[i-1][0], result[i][0]) in bigrams:\n","                    if -math.log(add_one_smoothing((result[i-1][0], result[i][0]), word_counts))  < threshold or\\\n","                        (result[i-1][0], result[i][0]) in vocabulary:\n","                        result[i][1] = 'c'\n","            if i!=0 and i!=len(result) - 1:\n","                if (result[i-1][0], result[i][0]) in bigrams:\n","                    if -math.log(add_one_smoothing((result[i-1][0], result[i][0]), word_counts)) < threshold or\\\n","                        (result[i-1][0], result[i][0]) in vocabulary:\n","                        result[i][1] = 'c'\n","                if (result[i][0], result[i+1][0]) in bigrams:\n","                    if -math.log(add_one_smoothing((result[i][0], result[i+1][0]), word_counts)) < threshold or\\\n","                        (result[i][0], result[i+1][0]) in vocabulary:\n","                        result[i][1] = 'c'\n","\n","    return result"]},{"cell_type":"markdown","metadata":{"id":"Sw9IUIoPIiUO"},"source":["# Recall/Precision Score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9fd5HDPGIiUP"},"outputs":[],"source":["def recall_precision_score_detection(error_data, labels, word_counts, vocabulary, threshold = 10):\n","    detection = []\n","    correct_detect_error = 0\n","    detected_error = 0\n","    real_error = 0\n","    for err_sentence in error_data:\n","        detection.append(vnspell_Detector(\" \".join(err_sentence).strip(), word_counts, vocabulary, threshold))\n","    for sen in detection:\n","        for word in sen:\n","            if word[1] == 'i':\n","                detected_error += 1\n","\n","    for i in range(0, len(error_data)):\n","        #print(i)\n","        for j in range(0, len(error_data[i])):\n","            if labels[i][j] == detection[i][j][1] and labels[i][j] == \"i\":\n","                correct_detect_error += 1\n","\n","    for label in labels:\n","        real_error += label.count(\"i\")\n","\n","    precision = correct_detect_error/detected_error\n","    recall = correct_detect_error/real_error\n","    f1 = (2 * precision * recall) / (precision + recall)\n","    print(\"-------------------Dectection------------------\")\n","    print(\"number of errors detected: \", detected_error)\n","    print(\"real number of errors: \",real_error)\n","    print(\"number of errors detected correctly: \",correct_detect_error)\n","    #print(\"Precision: \",precision * 100.0)\n","    #print(\"Recall: \",recall * 100.0)\n","    #print(\"F1-score:\", f1*100.0)\n","\n","\n","    TP, FP, TN, FN = 0, 0, 0, 0\n","    for i in range(0, len(error_data)):\n","        #print(i)\n","        for j in range(0, len(error_data[i])):\n","            if detection[i][j][1] == 'i' and labels[i][j] == \"i\":\n","                TP += 1\n","            elif detection[i][j][1] == 'i' and labels[i][j] == \"c\":\n","                FP += 1\n","            elif detection[i][j][1] == 'c' and labels[i][j] == \"c\":\n","                TN += 1\n","            else:\n","                FN += 1\n","\n","    accuracy  = (TP + TN) / (TP + FP + TN + FN)\n","    precision1 = (TP) / (TP + FP)\n","    recall1    = (TP) / (TP + FN)\n","    f1_score  = (2 * precision1 * recall1) / (precision1 + recall1)\n","    #print(\"---------------------------------------------------------\")\n","    print(\"Accuracy: \", accuracy*100)\n","    print(\"Precision: \", precision1*100)\n","    print(\"Recall: \", recall1*100)\n","    print(\"F1-score: \", f1_score*100)\n","    print(\"|||||||||||||||||||||||||||||||||||||||||||||||||||||||||\")\n","    return accuracy, precision1, recall1, f1_score\n","\n","\n",""]},{"cell_type":"markdown","metadata":{"id":"Ft0iEE8XIiUR"},"source":["# Find Threshold"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":103},"id":"12Ez_tgoIiUR","outputId":"d651d6e4-4448-433f-f474-ef3d427b9896","executionInfo":{"status":"ok","timestamp":1683997439363,"user_tz":-420,"elapsed":3,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# t = \"E:\\\\Khóa luận\\\\Spell_Check_Program\\\\data\\\\test_data\\\\test1.tsv\"\\nt = \"/content/drive/MyDrive/Colab Notebooks/data/test_data/test1.1.tsv\"\\nerror_dataz ,corrected_dataz, labelsz = read_tsv(t)\\nloop = []\\naccuracy = []\\nprecision = []\\nrecall = []\\nf1_score = []\\nfor i in range(8, 15):\\n    threshold = i\\n    print(i)\\n    a, b, c, d = find_threshold(error_dataz , labelsz, word_counts, vocab, threshold)\\n    loop.append(i)\\n    accuracy.append(a)\\n    precision.append(b)\\n    recall.append(c)\\n    f1_score.append(d)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":42}],"source":["def find_threshold(error_data, labels, word_counts, vocab, threshold):\n","    return recall_precision_score_detection(error_data, labels, word_counts, vocab, threshold)\n","'''\n","# t = \"E:\\\\Khóa luận\\\\Spell_Check_Program\\\\data\\\\test_data\\\\test1.tsv\"\n","t = \"/content/drive/MyDrive/Colab Notebooks/data/test_data/test1.1.tsv\"\n","error_dataz ,corrected_dataz, labelsz = read_tsv(t)\n","loop = []\n","accuracy = []\n","precision = []\n","recall = []\n","f1_score = []\n","for i in range(8, 15):\n","    threshold = i\n","    print(i)\n","    a, b, c, d = find_threshold(error_dataz , labelsz, word_counts, vocab, threshold)\n","    loop.append(i)\n","    accuracy.append(a)\n","    precision.append(b)\n","    recall.append(c)\n","    f1_score.append(d)\n","'''\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137},"id":"eHWiXwZdIiUR","outputId":"2282641a-2cad-4a5e-a336-ca480ff30242","executionInfo":{"status":"ok","timestamp":1683997440020,"user_tz":-420,"elapsed":3,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# Importing packages\\nimport matplotlib.pyplot as plt\\n\\n# Plot a simple line chart\\nplt.plot(loop, accuracy, \\'g\\', label=\\'accuracy\\', marker=\"o\")\\n\\n# Plot another line on the same chart/graph\\nplt.plot(loop, precision, \\'r\\', label=\\'precision\\', marker=\"s\")\\n\\n# Plot another line on the same chart/graph\\nplt.plot(loop, recall, \\'b\\', label=\\'recall\\', marker=\"^\")\\n\\n# Plot another line on the same chart/graph\\nplt.plot(loop, f1_score, \\'y\\', label=\\'f1_score\\', marker=\"D\")\\n\\n# set label name of x-axis title\\nplt.xlabel(\"Threshold\")\\n\\n# set label name of x-axis title\\nplt.ylabel(\"Rate\")\\n\\n# set label name of chart title\\n#plt.title(\"Threshold\")\\nplt.grid(color = \\'black\\', linestyle = \\'--\\', linewidth = 0.5)\\nplt.legend()\\nplt.show()\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":43}],"source":["'''\n","# Importing packages\n","import matplotlib.pyplot as plt\n","\n","# Plot a simple line chart\n","plt.plot(loop, accuracy, 'g', label='accuracy', marker=\"o\")\n","\n","# Plot another line on the same chart/graph\n","plt.plot(loop, precision, 'r', label='precision', marker=\"s\")\n","\n","# Plot another line on the same chart/graph\n","plt.plot(loop, recall, 'b', label='recall', marker=\"^\")\n","\n","# Plot another line on the same chart/graph\n","plt.plot(loop, f1_score, 'y', label='f1_score', marker=\"D\")\n","\n","# set label name of x-axis title\n","plt.xlabel(\"Threshold\")\n","\n","# set label name of x-axis title\n","plt.ylabel(\"Rate\")\n","\n","# set label name of chart title\n","#plt.title(\"Threshold\")\n","plt.grid(color = 'black', linestyle = '--', linewidth = 0.5)\n","plt.legend()\n","plt.show()\n","'''"]},{"cell_type":"markdown","metadata":{"id":"zSZj9BaIIiUR"},"source":["# Get candidates"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rmCxwHKJIiUR"},"outputs":[],"source":["def get_word_candidates(err_word, counts, vocab):\n","    unigrams = counts[0]\n","    #err_word = get_most_equivalent(err_word)\n","    unique_words = []\n","    s = []\n","\n","    for i in vocab:\n","        unique_word = str(i[0])\n","        unique_words.append(unique_word)\n","        ed = edit_distance(err_word, unique_word)\n","        s.append(ed)\n","    distance = dict(zip(unique_words, s))\n","    dist_sorted = dict(sorted(distance.items(), key=lambda x:x[1]))\n","    #min_dist = list(dist_sorted.values())\n","    min_dist = set(dist_sorted.values())\n","\n","    # print(min_dist)\n","    #keys_min = list(filter(lambda k: dist_sorted[k] == min_dist[0], dist_sorted.keys()))\n","\n","    keys_min = []\n","\n","    # if 4 in min_dist:\n","    #    keys_min.extend(list(filter(lambda k: dist_sorted[k] == 4, dist_sorted.keys())))\n","    if 0 in min_dist:\n","        keys_min.extend(list(filter(lambda k: dist_sorted[k] == 0, dist_sorted.keys())))\n","    if 1 in min_dist:\n","        keys_min.extend(list(filter(lambda k: dist_sorted[k] == 1, dist_sorted.keys())))\n","    if 2 in min_dist:\n","        keys_min.extend(list(filter(lambda k: dist_sorted[k] == 2, dist_sorted.keys())))\n","    if 3 in min_dist:\n","        keys_min.extend(list(filter(lambda k: dist_sorted[k] == 3, dist_sorted.keys())))\n","\n","    return keys_min\n"]},{"cell_type":"markdown","metadata":{"id":"KQEUFDQdIiUS"},"source":["# Correct error"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DOulC32WIiUT","outputId":"f3ba7346-d7f7-4ac9-9ff6-d1f912f6306c","executionInfo":{"status":"ok","timestamp":1683997443827,"user_tz":-420,"elapsed":2,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]},{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":45}],"source":["def lcs(X, Y, m, n):\n","    if m == 0 or n == 0:\n","        return 0\n","    elif X[m-1] == Y[n-1]:\n","        return 1 + lcs(X, Y, m-1, n-1)\n","    else:\n","        return max(lcs(X, Y, m, n-1), lcs(X, Y, m-1, n))\n","print(lcs(\"cat\", \"dog\", len(\"cat\"), len(\"dog\")))\n","edit_distance(\"cat\", \"dog\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HaRhrZxCJj3S"},"outputs":[],"source":["def vnspell_Corrector(sentence, word_counts, vocabulary, threshold = 10):\n","    unigrams = word_counts[0]\n","    bigrams = word_counts[1]\n","    check_misspell = vnspell_Detector(sentence, word_counts, vocabulary)\n","    #print(check_misspell)\n","    detected_misspell = []\n","    idx = 0\n","    for item in check_misspell:\n","        if item[1] == 'i':\n","            detected_misspell.append([item[0], idx])\n","        else:\n","            detected_misspell.append([item[0], 'c'])\n","        idx += 1\n","    #print(detected_misspell)\n","    corrected = []\n","    for i in range(len(detected_misspell)):\n","        item = detected_misspell[i]\n","        #print(item)\n","        if item[1] == 'c':\n","            corrected.append(item)\n","        else:\n","            candidates = get_word_candidates(item[0], word_counts, vocabulary)[0:55] # edit distance\n","            #print(candidates)\n","\n","            err_and_sug = dict(zip((item), [candidates]))\n","            #print(err_and_sug)\n","            \"\"\"\n","            recommended_front = dict()\n","            recommended_back = dict()\n","            for suggestion in err_and_sug[item[0]]:\n","                if i == 0:\n","                    recommended_back[suggestion] = add_one_smoothing((suggestion, detected_misspell[i+1][0].lower()), word_counts)\n","                elif i == len(detected_misspell)-1:\n","                    recommended_front[suggestion] = add_one_smoothing((detected_misspell[i-1][0].lower(), suggestion), word_counts)\n","                else:\n","                    recommended_front[suggestion] = add_one_smoothing((detected_misspell[i-1][0].lower(), suggestion), word_counts)\n","                    recommended_back[suggestion] = add_one_smoothing((suggestion, detected_misspell[i+1][0].lower()), word_counts)\n","\n","            recommended_front_sorted = list(sorted(recommended_front.items(), key=lambda x:x[1], reverse=True))\n","            recommended_back_sorted = list(sorted(recommended_back.items(), key=lambda x:x[1], reverse=True))\n","\n","            recommended_front_word = [word[0] for word in recommended_front_sorted]\n","            recommended_back_word = [word[0] for word in recommended_back_sorted]\n","            #print(recommended_front_sorted)\n","            #print(recommended_back_sorted)\n","            #print(recommended_front_word[0:3])\n","            #print(recommended_back_word[0:3])\n","            \"\"\"\n","            #combine = set(recommended_front_word + recommended_back_word)\n","            #print(combine)\n","\n","            max = 0\n","            for w in candidates:\n","                lcs_value = lcs(item[0], w, len(item[0]), len(w))\n","                if lcs_value > max:\n","                    max = lcs_value\n","            #print(max)\n","            longest = [max-1, max]\n","            best_choice = []\n","            #for word in combine:\n","            for word in candidates:\n","                lcs_value_new = lcs(item[0], word, len(item[0]), len(word))\n","                if i == 0:\n","                    next = add_one_smoothing((word, detected_misspell[i+1][0].lower()), word_counts)\n","                    if (-math.log(next) < threshold\\\n","                        or (word, detected_misspell[i+1][0].lower()) in vocab) and lcs_value_new in longest:\n","                        best_choice.append(tuple([word, next]))\n","                elif i == len(detected_misspell)-1:\n","                    prev = add_one_smoothing((detected_misspell[i-1][0].lower(), word), word_counts)\n","                    if (-math.log(prev) < threshold\\\n","                        or (detected_misspell[i-1][0].lower(), word) in vocab) and lcs_value_new in longest:\n","                        best_choice.append(tuple([word, prev]))\n","                else:\n","                    prev = (add_one_smoothing((detected_misspell[i-1][0].lower(), word), word_counts))\n","                    next = (add_one_smoothing((word, detected_misspell[i+1][0].lower()), word_counts))\n","                    if (-math.log(prev) < threshold or -math.log(next) < threshold\\\n","                        or (detected_misspell[i-1][0].lower(), word) in vocab\\\n","                        or (word, detected_misspell[i+1][0].lower()) in vocab) and lcs_value_new in longest:\n","                        if prev > next:\n","                            best_choice.append(tuple([word, prev]))\n","                        else:\n","                            best_choice.append(tuple([word, next]))\n","\n","            #print(best_choice)\n","            best_choice.sort(key = lambda x: x[1], reverse=True)\n","            suggestions = [item[0] for item in best_choice]\n","            #print(suggestions)\n","            item.extend(suggestions)\n","            #corrected.append([item, suggestions])\n","            corrected.append(item)\n","\n","\n","    #print(check_misspell)\n","    #print(detected_misspell)\n","    #print(corrected)\n","    res = []\n","    for item in corrected:\n","        if str(item[1]) != 'c':\n","            if len(item) != 2:\n","                res.append([item[2],'i'])\n","            else:\n","                res.append([item[0],'i'])\n","        else:\n","            res.append([item[0], 'c'])\n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SsbEVn6QsztQ"},"outputs":[],"source":["def vnspell_Corrector_suggestions(sentence, word_counts, vocabulary, threshold = 10, num_suggestions = 5):\n","    unigrams = word_counts[0]\n","    bigrams = word_counts[1]\n","    check_misspell = vnspell_Detector(sentence, word_counts, vocabulary)\n","    #print(check_misspell)\n","    detected_misspell = []\n","    idx = 0\n","    for item in check_misspell:\n","        if item[1] == 'i':\n","            detected_misspell.append([item[0], idx])\n","        else:\n","            detected_misspell.append([item[0], 'c'])\n","        idx += 1\n","    #print(detected_misspell)\n","    corrected = []\n","    for i in range(len(detected_misspell)):\n","        item = detected_misspell[i]\n","        #print(item)\n","        if item[1] == 'c':\n","            corrected.append(item)\n","        else:\n","            candidates = get_word_candidates(item[0], word_counts, vocabulary)[0:55] # edit distance\n","            #print(candidates)\n","\n","            err_and_sug = dict(zip((item), [candidates]))\n","            #print(err_and_sug)\n","            \"\"\"\n","            recommended_front = dict()\n","            recommended_back = dict()\n","            for suggestion in err_and_sug[item[0]]:\n","                if i == 0:\n","                    recommended_back[suggestion] = add_one_smoothing((suggestion, detected_misspell[i+1][0].lower()), word_counts)\n","                elif i == len(detected_misspell)-1:\n","                    recommended_front[suggestion] = add_one_smoothing((detected_misspell[i-1][0].lower(), suggestion), word_counts)\n","                else:\n","                    recommended_front[suggestion] = add_one_smoothing((detected_misspell[i-1][0].lower(), suggestion), word_counts)\n","                    recommended_back[suggestion] = add_one_smoothing((suggestion, detected_misspell[i+1][0].lower()), word_counts)\n","\n","            recommended_front_sorted = list(sorted(recommended_front.items(), key=lambda x:x[1], reverse=True))\n","            recommended_back_sorted = list(sorted(recommended_back.items(), key=lambda x:x[1], reverse=True))\n","\n","            recommended_front_word = [word[0] for word in recommended_front_sorted]\n","            recommended_back_word = [word[0] for word in recommended_back_sorted]\n","            #print(recommended_front_sorted)\n","            #print(recommended_back_sorted)\n","            #print(recommended_front_word[0:3])\n","            #print(recommended_back_word[0:3])\n","            \"\"\"\n","            #combine = set(recommended_front_word + recommended_back_word)\n","            #print(combine)\n","\n","\n","            max = 0\n","            for w in candidates:\n","                lcs_value = lcs(item[0], w, len(item[0]), len(w))\n","                if lcs_value > max:\n","                    max = lcs_value\n","            #print(max)\n","            longest = [max-1, max]\n","\n","            best_choice = []\n","            #for word in combine:\n","            for word in candidates:\n","                lcs_value_new = lcs(item[0], word, len(item[0]), len(word))\n","                if i == 0:\n","                    next = add_one_smoothing((word, detected_misspell[i+1][0].lower()), word_counts)\n","                    if (-math.log(next) < threshold\\\n","                        or (word, detected_misspell[i+1][0].lower()) in vocab) and lcs_value_new in longest:\n","                        best_choice.append(tuple([word, next]))\n","                elif i == len(detected_misspell)-1:\n","                    prev = add_one_smoothing((detected_misspell[i-1][0].lower(), word), word_counts)\n","                    if (-math.log(prev) < threshold\\\n","                        or (detected_misspell[i-1][0].lower(), word) in vocab) and lcs_value_new in longest:\n","                        best_choice.append(tuple([word, prev]))\n","                else:\n","                    prev = (add_one_smoothing((detected_misspell[i-1][0].lower(), word), word_counts))\n","                    next = (add_one_smoothing((word, detected_misspell[i+1][0].lower()), word_counts))\n","                    if (-math.log(prev) < threshold or -math.log(next) < threshold\\\n","                        or (detected_misspell[i-1][0].lower(), word) in vocab\\\n","                        or (word, detected_misspell[i+1][0].lower()) in vocab) and lcs_value_new in longest:\n","                        if prev > next:\n","                            best_choice.append(tuple([word, prev]))\n","                        else:\n","                            best_choice.append(tuple([word, next]))\n","\n","            #print(best_choice)\n","            best_choice.sort(key = lambda x: x[1], reverse=True)\n","            suggestions = [item[0] for item in best_choice][0:num_suggestions]\n","            #print(suggestions)\n","            # item.extend(suggestions)\n","            #corrected.append([item, suggestions])\n","            corrected.append([item, suggestions])\n","\n","\n","    #print(check_misspell)\n","    #print(detected_misspell)\n","    return corrected\n",""]},{"cell_type":"markdown","metadata":{"id":"wLiTLgyWKmyS"},"source":["# Accuracy of Correction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DSp1nuIXIiUU"},"outputs":[],"source":["def recall_precision_score_correction(error_data, corrected_data, word_counts, vocabulary, threshold = 10, num_suggestions=5):\n","    correction = []\n","\n","    for err_sentence in error_data:\n","        # correction.append(vnspell_Corrector(\" \".join(err_sentence).strip(), word_counts, vocabulary, threshold))\n","        correction.append(vnspell_Corrector_suggestions(\" \".join(err_sentence).strip(), word_counts, vocabulary, threshold, num_suggestions))\n","\n","    F = 0\n","    T = 0\n","    err_tokens = 0\n","    for i in range(0, len(error_data)):\n","        #print(i)\n","        for j in range(0, len(error_data[i])):\n","            # if correction[i][j][1] == 'i':\n","            if type(correction[i][j][0]) == list:\n","                err_tokens += 1\n","                # if correction[i][j][0] == corrected_data[i][j]:\n","                if len(correction[i][j][1]) == 0:\n","                  if correction[i][j][0][0] == corrected_data[i][j]:\n","                    T += 1\n","                  else:\n","                    F += 1\n","                else:\n","                  if corrected_data[i][j] in correction[i][j][1]:\n","                    T += 1\n","                      #print(correction[i][j][0], corrected_data[i][j])\n","                  else:\n","                    F += 1\n","                      #print(correction[i][j][0], corrected_data[i][j])\n","\n","    accuracy = T/err_tokens\n","    print(\"--------------------Correction-----------------------\")\n","    print(\"Accuracy: \", accuracy*100, '%')\n","    print(\"Num err_token: \", err_tokens)\n","    print(\"True Correction: \", T)\n","    print(\"False Correction: \", F)\n","    print(\"|||||||||||||||||||||||||||||||||||||||||||||||||||||||||\")\n","    #return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"trCdObnSKyxR","outputId":"77623f2f-f603-43cf-889d-f61def66a660","executionInfo":{"status":"ok","timestamp":1683986181587,"user_tz":-420,"elapsed":34238284,"user":{"displayName":"Ngọc Tỉnh Nguyễn","userId":"11127003306068690640"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------Dectection------------------\n","number of errors detected:  24002\n","real number of errors:  24358\n","number of errors detected correctly:  18765\n","Accuracy:  96.12200466934986\n","Precision:  78.1809849179235\n","Recall:  77.0383446916824\n","F1-score:  77.60545905707194\n","|||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n","--------------------Correction-----------------------\n","Accuracy:  73.26889425881177 %\n","Num err_token:  24002\n","True Correction:  17586\n","False Correction:  6416\n","|||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n"]}],"source":["# t = \"/content/drive/MyDrive/Colab Notebooks/3.tsv\"\n","# t = \"E:\\\\Khóa luận\\\\Spell_Check_Program\\\\data\\\\test_data\\\\New folder\\\\1.txt\"\n","# error_dataz ,corrected_dataz, labelsz = read_tsv(t)\n","# recall_precision_score_detection(error_dataz, labelsz, word_counts, vocab)\n","# recall_precision_score_correction(error_dataz, corrected_dataz, word_counts, vocab, threshold=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OtA-03M4u9eU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cbfbae59-38cd-4914-f8e3-5a8d7617b051"},"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------Dectection------------------\n","number of errors detected:  24002\n","real number of errors:  24358\n","number of errors detected correctly:  18765\n","Accuracy:  96.12200466934986\n","Precision:  78.1809849179235\n","Recall:  77.0383446916824\n","F1-score:  77.60545905707194\n","|||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n"]}],"source":["t = \"/content/drive/MyDrive/Colab Notebooks/3.tsv\"\n","# t = \"E:\\\\Khóa luận\\\\Spell_Check_Program\\\\data\\\\test_data\\\\New folder\\\\1.txt\"\n","error_dataz ,corrected_dataz, labelsz = read_tsv(t)\n","recall_precision_score_detection(error_dataz, labelsz, word_counts, vocab)\n","recall_precision_score_correction(error_dataz, corrected_dataz, word_counts, vocab, threshold=10, num_suggestions=1)"]},{"cell_type":"code","source":[],"metadata":{"id":"U6ISk-kgEg7g"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"interpreter":{"hash":"92bf770d7976fbe70d07e31b0539d10b425d8d2f9f3862b5215defe2cfd843d6"},"kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}